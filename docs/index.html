<head>
   <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-DB5J2MQV0D"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DB5J2MQV0D');
    </script>

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
    google.load("jquery", "1.3.2");
    </script>    
</head>

<style type="text/css">
body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
}

h1 {
    font-weight: 300;
    margin: 0.4em;
}

/* p {
    margin: 0.2em;
} */

.disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    padding: 20px;
}

video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

a:link,
a:visited {
    color: #1367a7;
    text-decoration: none;
}

a:hover {
    color: #208799;
}

td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
}

.layered-paper-big {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
    /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
}


.layered-paper {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
    /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
}

.vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
}

hr {
    margin: 0;
    border: 0;
    height: 1.5px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
}

.rotate {
    /* FF3.5+ */
    -moz-transform: rotate(-90.0deg);
    /* Opera 10.5 */
    -o-transform: rotate(-90.0deg);
    /* Saf3.1+, Chrome */
    -webkit-transform: rotate(-90.0deg);
    /* IE6,IE7 */
    filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);
    /* IE8 */
    -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)";
    /* Standard */
    transform: rotate(-90.0deg);
}

c {
    white-space: nowrap;
    writing-mode: tb-rl;
    transform: rotate(-180.0deg);
}

    .topnav {
      background-color: #eeeeee;
      overflow: hidden;
    }

    .topnav div {
      max-width: 1070px;
      margin: 0 auto;
    }

    .topnav a {
      display: inline-block;
      color: black;
      text-align: center;
      vertical-align: middle;
      padding: 16px 16px;
      text-decoration: none;
      font-size: 16px;
    }

    .topnav img {
      width: 100%;
      margin: 0.2em 0px 0.3em 0px;
    }
    .authors div{
        text-align: center;
    }
    .content{
        margin-bottom: 2em;
        font-size: 12pt;
    }
    p {
        display: block;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0px;
        margin-inline-end: 0px;
    }

.bibtex pre
{
    margin-bottom: 0;
    font-family: Consolas, Monaco, monospace;
    white-space: pre-wrap; /* CSS 3 */
    white-space: -moz-pre-wrap; /* Mozilla, since 1999 */
    white-space: -pre-wrap; /* Opera 4-6 */
    white-space: -o-pre-wrap; /* Opera 7 */
    word-wrap: break-word; /* IE 5.5+ */
    width: 100%;
    color: #444;
    padding: 0px;
    background: rgba(238, 238, 238, 0.623);
    border: 1px solid #ccc;
    overflow: auto;
}

</style>
<html>

<head>
    <title>InterScene</title>
    <meta property="og:title" content="nlos" />
</head>

<body>	

    <br>
    <center>
    <p>
        <span style="font-size:42px"> Synthesizing Physically Plausible Human Motions <br> in 3D Scenes </span>
    </p>
    </center>
    <br>
    <div  align=center class="authors">
       <a href="https://liangpan99.github.io/"> Liang Pan<sup>1</sup></a> 
       &nbsp;  
       <a href="https://wangjingbo1219.github.io/"> Jingbo Wang<sup>2</sup></a> 
       &nbsp;  
       <a href="http://www.buzhenhuang.com/"> Buzhen Huang<sup>1</sup></a> 
       &nbsp;  
       Junyu Zhang<sup>1</sup>
       <br>
       <a href="https://haofanwang.github.io/"> Haofan Wang<sup>3</sup></a> 
       &nbsp;  
       <a href="https://tangxuvis.github.io/"> Xu Tang<sup>3</sup></a> 
       &nbsp;  
       <a href="https://www.yangangwang.com/"> Yangang Wang<sup>1</sup></a> 
      
    </div>
    <br>

    <div> 
        <table align=center width=500px>
            <tr>
                <td>
                    Southeast University<sup>1</sup> 
                </td>
                <td>
                    Shanghai AI Lab<sup>2</sup> 
                </td>
                <td>
                    Xiaohongshu Inc.<sup>3</sup> 
                </td>
            </tr>
        </table>
       
    </div>

    <br>

    <div align=center>
        arXiv Preprint 2023
    </div>
    
    
    <br>
    <div align=center>
        <a href="https://arxiv.org/abs/2308.09036"> [Paper]</a> &nbsp <a href="https://github.com/liangpan99/InterScene"> [Code]</a>
    </div>
    <br>

    <table align=center width=800px>
    <tr><td>
    <div  align=center class="content" width=400px>
    <img height=300 src="assets/teaser.png"> </img>
    
    <br>
    <p align="justify">
        Synthesizing physically plausible human motions in 3D scenes is a challenging problem. 
        Kinematics-based methods cannot avoid inherent artifacts (e.g., penetration and foot skating) due to the lack of physical constraints. 
        Meanwhile, existing physics-based methods cannot generalize to multi-object scenarios since the policy trained with reinforcement learning has limited modeling capacity. 
        In this work, we present a framework that enables physically simulated characters to perform long-term interaction tasks in diverse, cluttered, and unseen scenes. 
        The key idea is to decompose human-scene interactions into two fundamental processes, <strong>Inter</strong>acting and <strong>Nav</strong>igating, which motivates us to construct two reusable <strong>Con</strong>troller, 
        i.e., <strong>InterCon</strong> and <strong>NavCon</strong>. 
        Specifically, InterCon contains two complementary policies that enable characters to enter and leave the interacting state (e.g., sitting on a chair and getting up). 
        To generate interaction with objects at different places, we further design NavCon, a trajectory following policy, to keep characters' locomotion in the free space of 3D scenes. 
        Benefiting from the divide and conquer strategy, we can train the policies in simple environments and generalize to complex multi-object scenes. 
        Experimental results demonstrate that our framework can synthesize physically plausible long-term human motions in complex 3D scenes.
    </p>
    </div>
    </td></tr>
    </table>

    <center>
        <h1>Motivation</h1>
    </center>
    <hr>
    <br>
    <table align="center" width=900px>
        <tr>
        <td colspan='2'>
            <center>
            <video width="450" controls muted autoplay loop>
                <source src="./assets/motivation/multiobject_existing.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        <td>
            <center>
                <video width="450"  controls muted autoplay loop>
                <source src="./assets/motivation/multiobject_ours.mp4" type="video/mp4">
            </video>
            </center>
        </td> 
        </tr>

        <tr>
            <td colspan='2'>
                <center>
                <video width="450" controls muted autoplay loop>
                    <source src="./assets/motivation/obstacle_existing.mp4" type="video/mp4">
                </video>
                </center>
                <br>
                <center>
                    Previous works [<a href="https://arxiv.org/abs/1908.07423">1</a>, <a href="https://xbpeng.github.io/projects/InterPhys/index.html">2</a>]
                </center>
            </td>
            <td>
                <center>
                    <video width="450"  controls muted autoplay loop>
                    <source src="./assets/motivation/obstacle_ours.mp4" type="video/mp4">
                </video>
                </center>
                <br>
                <center>
                    Ours
                </center>
            </td> 
        </tr>

    <table align="center" width=900px>
        <tr>
            <td>
                <p>
                Existing physics-based frameworks cannot generalize to multi-object scenarios 
                due to the lack of two important abilities: <br> (1-top) continuous interaction, (2-bottom) obstacle avoidance.
                </p>
            </td>
        </tr>
    </table>
    
    <center>
        <h1>Pipeline</h1>
    </center>  
    <hr>
    <br>
    <table  align=center width=900px>
        <tr>
        <td>
            <img align="center" width="900px" src="assets/pipeline.png"></img>
        </td>
        </tr>
        <tr>
        <td>
            <p>
                The interaction controller consists of two separate control policies, which provide two interaction-involved skills, i.e., sitting and getting up. 
                The navigation controller employs a trajectory following policy that controls the character's movements along a specific path. 
                Then, two reusable controllers are combined to synthesize human motions in complex 3D scenes without additional training. 
                This is achieved by using a finite state machine that receives user instructions to enable the simulated character to perform long-term interaction tasks.
            </p>
        </td>
        </tr>

    </table>


    <center>
        <h1>Synthesized Results in Diverse 3D Scenes</h1>
    </center>
    <hr>
    <br>
    <table align="center" width=900px>
        <tr>
        <td colspan='2'>
            <center>
            <video width="450" controls muted autoplay loop>
                <source src="./assets/results/scene_0_motion_0.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        <td>
            <center>
                <video width="450"  controls muted autoplay loop>
                <source src="./assets/results/scene_0_motion_1.mp4" type="video/mp4">
            </video>
            </center>
        </td> 
        </tr>
    </table>

    <table align="center" width=900px>
        <tr>
        <td colspan='2'>
            <center>
            <video width="450" controls muted autoplay loop>
                <source src="./assets/results/scene_1_motion_0.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        <td>
            <center>
                <video width="450"  controls muted autoplay loop>
                <source src="./assets/results/scene_1_motion_1.mp4" type="video/mp4">
            </video>
            </center>
        </td> 
        </tr>
    </table>

    <table align="center" width=900px>
        <tr>
        <td colspan='2'>
            <center>
            <video width="450" controls muted autoplay loop>
                <source src="./assets/results/scene_2_motion_0.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        <td>
            <center>
                <video width="450"  controls muted autoplay loop>
                <source src="./assets/results/scene_2_motion_1.mp4" type="video/mp4">
            </video>
            </center>
        </td> 
        </tr>
    </table>

    <center>
        <h1>Scalability of Our Framework</h1>
    </center>
    <hr>
    <br>
    <table align="center" width=900px>
        <tr>
            <td>
                <img width="450" src="assets/scalability/lie.png"> </img>
            </td> 
        <td colspan='2'>
            <center>
            <video width="450" controls muted autoplay loop>
                <source src="./assets/scalability/sitting_plus_lying.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        </tr>
    </table>

    <table align="center" width=900px>
        <tr>
            <td>
                By training an additional interaction controller, our framework can be extended to new action (e.g., lying down), 
                which enables physically simulated characters to interaction with objects more diversely.
            </td> 
        </tr>
    </table>

    <center>
        <h1>Citation</h1>
    </center>
    <hr>

    <div class="section bibtex">
        <pre>
@misc{pan2023synthesizing,
    title={Synthesizing Physically Plausible Human Motions in 3D Scenes}, 
    author={Liang Pan and Jingbo Wang and Buzhen Huang and Junyu Zhang and Haofan Wang and Xu Tang and Yangang Wang},
    year={2023},
    eprint={2308.09036},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}      
        </pre>
    </div>



    <center>
        <h1>Related Projects</h1>
    </center>
    <hr>
    <p>
        <ul> 
            <li>[1] <a href="https://arxiv.org/abs/1908.07423">Learning to Sit</a>: Synthesizing Human-Chair Interactions via Hierarchical Control, AAAI 2021.</li>
            <li>[2] <a href="https://xbpeng.github.io/projects/InterPhys/index.html">InterPhys</a>: Synthesizing Physical Character-Scene Interactions, SIGGRAPH 2023.</li>
            <li>[3] <a href="https://xbpeng.github.io/projects/AMP/index.html">AMP</a>: Adversarial Motion Priors for Stylized Physics-Based Character Control, SIGGRAPH 2021.</li>
            <li>[4] <a href="https://research.nvidia.com/labs/toronto-ai/trace-pace/">Trace and Pace</a>: Controllable Pedestrian Animation via Guided Trajectory Diffusion, CVPR 2023.</li>
        </ul>
        
        We sincerely thank <a href="https://geometry.stanford.edu/projects/humor/">HuMoR</a> for its awesome renderer. This project page template is based on <a href="https://nileshkulkarni.github.io/nifty/">this page</a>. <br>
    </p>
    <br>
    
    

</body>

</html>